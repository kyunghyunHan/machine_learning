# machine_learning

## 개념

- 인공지능 > 머신러넝 >딥러닝
- 인공지능 : 인간의 지능을 모방 문제해결을 위해 사람처럼 학습
- 인간의 학습능력을 기계를 통해 구현하는 분야
- 심층 신경망 기반의 머신러닝 방법

## 처리과정

- 학습단계 - 추론단계

## 기본요소

- n차원의 열벡터로 표현
- 특징추출 → 데이터에서 불필요한 정보를 제거하고 처리를 위한 핵심적 정보를 얻는 것
- 목적함수 → 주어진 데이터 집합을 이용하여 학습 시스템이 달성해야 하는 목표를 기계가 알 수 있는 수학적 함수로 정의한 것
- 오차함수 → 학습 시스템의 출력과 원하는 출력의 차이(“오차”)로 정의되는 목적함수
- 성능 평가 기준 → 학습오차, 테스트 오차, 일반화 오차
- 교차검증법 → 제한된 데이터 집합을 이용하여 일반화 오차에 좀 더 근접한 오차값을 얻어 내는 방법

## 주제

- 주제:분류,회귀,군집화,특징추출
- 분류:입력데이터가 어떤 부류에 속하는지 자동으로 판단하는 문제
- 회귀:학습을 통해 입력변수와 원하는 출력변수 사이의 매핑 관계를 찾는것
- 군집화:주어지는 클래스 정보 없이 단순히 하나의 덩어리로 이루어진 데이터를 받아서 데이터의 데이터의 성질 또는 분포 특성 등을 바탕으로 유사한 데이터끼리 묶어서 임의로 복수 개의 클러스터로 만드는것

## 학습

- 지도학습
- 비지도학습
- 강화학습
- 과다적합

## 선형회귀

- 역학에서 금융 및 천문학에 널리사용
- GLM참조

## 분류

- 분류 → 주어진 데이터 집합에 대해 이미 정의된 몇 개의 클래스(부류)로 입력을 구분하는 문제 → 목표 출력값을 사용하여 학습을 진행하는 지도학습을 적용
- 분류기의 종류 → 베이즈 분류기, K–최근접이웃 분류기, 로지스틱 회귀, 결정 트리, SVM, 신경망 등

## 베이즈 분류기

- 주어진 데이터가 각 클래스로부터 생성되었을 후험확률(사후확률)를 계산하고, 그 값이 가장 큰 클래스로 분류를 수행 → 후험확률은 학습 데이터를 이용하여 추정된 클래스별 분포함수를 이용하여 계산됨
- 우도비 분류 → 각 클래스에서 데이터가 관찰된 확률밀도의 비율(“우도비”)에 의한 분류
- 베이즈 분류기 → 후험확률에 대한 베이즈 정리로부터 유도된 판별함수를 이용하여 분류하는 방식
- 이진 분류 문제의 경우 두 확률밀도함수의 곡선이 만나는 지점이 오류의 확률이 최소인 결정경계가 되며, 전체 데이터 집합에서 각 클래스가 차지하는 비율에 따라서 결정경계가 조정됨
- 클래스별 확률밀도함수가 가우시안 분포를 따르는 경우 공분산행렬의 형태에 따라 결정경계와 판별함수가 달라짐 → 최소거리 분류기, 마할라노비스 거리, 정규화된 유클리디안 거리

## K–최근접이웃 분류기

최근접이웃 분류기 → 클래스와 상관없이 모든 데이터 중 가장 작은 거리값을 가지는 데이터를 찾아 그 데이터가 속하는 클래스로 할당하는 방법 → “K=1인 K-최근접이웃 분류기”

- K–최근접이웃 분류기(K–NN) → 주어진 데이터로부터 거리가 가까운 순서대로 K개의 데이터를 찾은 후, 그중 가장 많은 수의 데이터가 속한 클래스로 할당하는 방법 → 분류 수행을 위해 항상 학습 데이터를 저장해야 함 → 비용(계산량, 메모리) 문제 발생
- K–NN 분류기 → 매우 비선형적인 결정경계를 가지며, 복잡한 데이터 분포에 대해서 비교적 잘 작동 → 분류를 위해서는 학습 데이터를 항상 저장해야 하는 문제점을 가짐.
- 가우시안 베이즈 분류기 → 각 클래스에 대해 가우시안 분포를 미리 가정하고 파라미터를 추정하므로, 잘못된 가정에 따른 성능 저하의 위험성이 존재 → 학습 데이터로부터 각 클래스의 평균과 공분산을 계산한 후에는 학습 데이터가 더 이상 필요하지 않음.
- K–NN 분류기의 설계 고려사항 → 적절한 K값과 거리 함수의 선택
- 자주 쓰이는 거리 함수의 종류 → 2차 노름(유클리디안 거리), 1차 노름, p차 노름, 내적, 코사인 거리, 정규화된 유클리디안 거리, 마할라노비스 거리 등
## 회귀
- 입력변수와 출력변수 사이의 매핑 관계를 찾는 것
## 군집화
- 비지도학습
- 서로소인 부분집합
- k-평균 군집화,가우시안
- 데이터에 대한 클래스 레이블이 주어지지 않는 경우

## k-평균 군집화 알고리즘
- 주어진 데이터 집합을 k개의 그룹이로 묶는 알고리즘
- 시작 - 데이터그룹핑 - 대표벡터수정- 반복여부 결정
- 수정전의 대표벡터와 수정후의 대표벡터의 차이를 계산해서 차이가 없으면 종료
- 결정경계
- 

## 계층적 군집화
- 병합적방법
- 분할적방법

## 특징추출
1. 선형변환에 의한 특징추출
- 특징추출 → n차원의 입력 벡터에 대해 변환함수를 적용하여 m차원의 특징벡터를 얻는 변환
- 선형변환에 의한 특징추출 Y=WTX → 주어진 데이터 X를 변환행렬 W에 의해 정해지는 방향으로 사영함으로써 저차원 특징값 Y를 얻는 것 → 변환행렬을 적절히 조정함으로써 분석 목적에 맞는 특징을 추출하는 것이 필요
- 선형변환에 의한 대표적인 통계적 특징추출 방법 → 주성분분석(PCA), 선형판별분석(LDA)
2. 주성분분석법
- 목적 → 변환 전의 데이터 X가 가지고 있는 정보를 차원 축소 후에도 최대한 유지하도록 하는 것
- 변환행렬 W를 어떻게 찾는가? → 데이터 손실량을 최소로 하는 사영 벡터를 찾음 → 데이터 집합의 분산이 가장 큰 방향을 찾음 → 데이터의 공분산행렬에 대한 고유치 분석을 통해 고유치와 고유벡터를 찾아 고유치가 가장 큰 값부터 순서대로 m개에 대응하는 고유벡터로 변환행렬을 구성함
- 알고리즘의 수행 단계
① 입력 데이터의 평균과 공분산을 계산함
② 공분산에 대한 고유치 분석을 통해 고유치행렬과 고유벡터행렬을 계산함
③ 고유치가 큰 것부터 순서대로 m개의 고유치를 선택함
④ 선택한 고유치에 대응되는 고유벡터를 열벡터로 가지는 변환행렬을 생성함
⑤ 생성된 변환행렬에 의한 선형변환으로 특징 데이터를 얻음
- 특징 → 데이터 분석에 대한 특별한 목적이 없는 경우에 유용. 클래스 레이블 정보를 활용하지 않는 비지도학습에 해당. 데이터 분포가 비선형적인 경우에는 부적합
3. 선형판별분석법
- 클래스 레이블 정보를 적극 활용하여 클래스 간의 거리는 가능한 멀어지게 하고, 같은 클래스 내에서는 결집되도록 하여 분류에 적합한 특징으로의 변환을 유도함
- 알고리즘의 수행 단계
① 입력 데이터를 각 클래스 레이블에 따라 M개의 클래스로 나누어 각각 평균과 클래스 간 산점행렬 SB, 그리고 클래스 내 산점행렬 SW을 계산함
② 행렬 SW-1SB 의 고유치 분석을 통해 고유치행렬과 고유벡터행렬을 계산함
③ 고유치가 큰 것부터 순서대로 m개의 고유치를 선택함
④ 선택한 고유치에 대응되는 고유벡터를 열벡터로 가지는 변환행렬을 생성함
⑤ 생성된 변환행렬에 의한 선형변환으로 특징 데이터를 얻음
- 특징 → 데이터가 복잡한 비선형 구조를 가진 경우에는 부적합. 고유치 분석을 통해 찾아지는 고유벡터의 개수가 제한됨(클래스의 개수가 M이라면 특징벡터의 차원도 최대 M-1로 제한됨). 데이터의 수가 입력 차원보다 크지 않으면 클래스 내 산점행렬의 역행렬을 구할 수 없어 적용 불가
4. 거리 기반 차원 축소 방법
- 두 데이터 간의 거리(또는 유사도)를 핵심 정보로 사용하여 차원을 축소하는 방법 → 현재 주어진 데이터 간의 거리가 추출된 저차원 특징들 사이에서도 최대한 유지하는 방향으로 차원 축소
- 거리의 정의에 따라 매우 다양한 방법이 있음 → 󰄤 MDS, t–SNE, Isomap 등
- 다차원 척도법(MDS): 원래 데이터 쌍의 거리와 추출된 특징 쌍의 유클리디안 거리에 대한 분석을 통해 차원 축소
- t-SNE → 확률밀도함수의 조건부확률을 이용하여 유사도를 정의하며, 특히 추출된 특징 쌍의 유사도를 정의할 때 t–분포를 사용
- Isomap → 기하학적 다양체(매니폴드)상에서의 측지 거리를 사용하는 차원 축소 방법
- 특징 → 특징값을 얻기 위하여 입력 데이터와 특징 데이터 간의 매핑 함수를 정의하지 않으므로 새로운 데이터에 대해서는 특징값을 찾지 못함 → 데이터 시각화의 용도로 주로 사용